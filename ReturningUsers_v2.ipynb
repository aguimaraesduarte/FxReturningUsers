{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta, datetime\n",
    "from pyspark.sql.functions import collect_list, substring\n",
    "from pyspark.sql import Row\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import ujson as json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int2date(n):\n",
    "    \"\"\"\n",
    "    This function converts a number of days since Jan 1st 1970 <n> to a date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return date(1970,1,1)+timedelta(days=n)\n",
    "    except:\n",
    "        return date(1970,1,1)\n",
    "\n",
    "def date2int(d):\n",
    "    \"\"\"\n",
    "    This function converts a date <d> to number of days since Jan 1st 1970.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (d-date(1970,1,1)).days\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def str2date(s, f=\"%Y%m%d\"):\n",
    "    \"\"\"\n",
    "    This function converts a string <s> in the format <f> to a date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(s, f).date()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# Define some global variables\n",
    "TODAY = date.today()                                    # today (date)\n",
    "TODAY_INT = date2int(TODAY)                             # today (days since Jan 1st, 1970)\n",
    "PCD_CUTOUT_START_DATE = date(2010, 1, 1)                # profiles created before this date are removed (date)\n",
    "PCD_CUTOUT_START_INT = date2int(PCD_CUTOUT_START_DATE)  # profiles created before this date are removed (days since Jan 1st, 1970)\n",
    "\n",
    "T0 = 30                                                 # days of latency\n",
    "T1 = 90                                                 # days to come back\n",
    "T2 = 180                                                # maximum date to go back in the past for tsla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_main_summary():\n",
    "    \"\"\"\n",
    "    This function imports the main_summary dataset from S3, selects the variables of interest,\n",
    "    applies several filters, and returns the filtered dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # connect to the main_summary dataset\n",
    "    allPingsDF = sqlContext.read.load(\"s3://telemetry-parquet/main_summary/v3\", \"parquet\", mergeSchema=True)\n",
    "    \n",
    "    # perform variable selection with column renaming\n",
    "    allPingsDFSelect = allPingsDF.select(\n",
    "               allPingsDF.client_id.alias(\"cid\"),\n",
    "               allPingsDF.subsession_start_date.alias(\"ssd\"),\n",
    "               allPingsDF.sample_id.alias(\"sid\"),\n",
    "               allPingsDF.normalized_channel.alias(\"channel\"),\n",
    "               allPingsDF.app_name.alias(\"appname\"),\n",
    "               allPingsDF.subsession_length.alias(\"ssl\"),\n",
    "               allPingsDF.profile_creation_date.alias(\"pcd\")\n",
    "    )\n",
    "\n",
    "    # - 1% sample (sample_id is 42)\n",
    "    # - channel is release\n",
    "    # - application is Firefox\n",
    "    # - subsession length is positive and less than 24h\n",
    "    # - profile creation date is after Jan 1st, 2010 and before TODAY\n",
    "    filteredPingsDF = allPingsDFSelect.filter(allPingsDFSelect.sid == \"42\")\\\n",
    "                                      .filter(allPingsDFSelect.channel == \"release\")\\\n",
    "                                      .filter(allPingsDFSelect.appname == \"Firefox\")\\\n",
    "                                      .filter(allPingsDFSelect.ssl >= 0)\\\n",
    "                                      .filter(allPingsDFSelect.ssl <= 86400)\\\n",
    "                                      .filter(allPingsDFSelect.pcd >= PCD_CUTOUT_START_INT)\\\n",
    "                                      .filter(allPingsDFSelect.pcd <= TODAY_INT)\\\n",
    "                                      .select([\"cid\",\n",
    "                                               substring(\"ssd\", 0, 10).alias(\"ssd\")\n",
    "                                              ])\\\n",
    "                                      .cache()\n",
    "    \n",
    "    # return filtered dataframe\n",
    "    return filteredPingsDF\n",
    "\n",
    "def get_active_on_date(DF_str, day):\n",
    "    \"\"\"\n",
    "    This function creates and returns a table that is aggregated by client_id (cid) and subsession_start_date (ssd).\n",
    "    Only client_ids that were active on given date are kept.\n",
    "    For each tuple (cid, sd), the number of subsessions per day is aggregated.\n",
    "    NOTE: This could be changed. For example, we may not need to know that a user was active multiple times per day.\n",
    "    \n",
    "    @params:\n",
    "        DF_str: [string] name of the dataframe returned by aggregate_by_client_date(...)\n",
    "        day: [date] date for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT cid,\n",
    "           ssd,\n",
    "           count(*) as num_ssd\n",
    "    FROM {table}\n",
    "    WHERE cid IN\n",
    "    (\n",
    "        SELECT distinct cid\n",
    "        FROM {table}\n",
    "        WHERE ssd = '{day}'\n",
    "    )\n",
    "    GROUP BY cid, ssd\n",
    "    \"\"\".format(table=DF_str,\n",
    "               day=day.isoformat())\n",
    "    \n",
    "    active = spark.sql(query).cache()\n",
    "    \n",
    "    return active\n",
    "\n",
    "def make_longitudinal(aggregate_df):\n",
    "    \"\"\"\n",
    "    This function creates and returns a longitudinal dataframe from the aggregate dataframe grouped by client_id (cid).\n",
    "    Each Row from this dataframe contains the sequential information (lists) for:\n",
    "        - subsession_start_date (ssd),\n",
    "        - num_ssd (num_ssd)\n",
    "    for each cid.\n",
    "    \n",
    "    @params:\n",
    "        agg_subset: [dataframe] dataframe returned by get_active_on_date(...)\n",
    "    \"\"\"\n",
    "\n",
    "    longitudinal = aggregate_df.groupBy(\"cid\")\\\n",
    "                               .agg({\"ssd\": \"collect_list\",\n",
    "                                     \"num_ssd\": \"collect_list\"})\\\n",
    "                               .withColumnRenamed(\"collect_list(ssd)\",\"ssd\")\\\n",
    "                               .withColumnRenamed(\"collect_list(num_ssd)\", \"num_ssd\")\n",
    "    \n",
    "    return longitudinal\n",
    "\n",
    "def mapping(row, day):\n",
    "    \"\"\"\n",
    "    Applied to an RDD, this mapping function returns a tuple of 5 elements:\n",
    "        - cid\n",
    "        - ssd (sorted ascending through time)\n",
    "        - num_ssd (sorted according to ssd)\n",
    "        - tsla (time since last activity in days)\n",
    "        - tuna (time until next activity in days)\n",
    "    \n",
    "    @params:\n",
    "        row: [Row] a row from a longitudinal RDD that includes:\n",
    "            - cid: client_id\n",
    "            - ssd: subsession_start_date\n",
    "        day: [date] date of analysis\n",
    "    \n",
    "    @logic:\n",
    "        - sort the row based on ssd (ascending).\n",
    "        - if there was activity prior to D0, get tsla\n",
    "        - if there was activity after D0, get tuna\n",
    "            - if not, those are set to -1\n",
    "            - in case of badly formatted ssd, return -2\n",
    "    \"\"\"\n",
    "    \n",
    "    def sort_row(row):\n",
    "        # sort ssd by ascending ssd\n",
    "        zipped = sorted(zip(row.ssd, row.num_ssd), reverse=False)\n",
    "        ssd, num_ssd = zip(*zipped)\n",
    "        \n",
    "        # return reformatted Row\n",
    "        return Row(cid=row.cid,\n",
    "                   ssd=list(ssd),\n",
    "                   num_ssd=list(num_ssd)\n",
    "                  )\n",
    "    \n",
    "    tsla = -1                        # time since last activity (days)\n",
    "    tuna = -1                        # time until next activity (days)\n",
    "    \n",
    "    sorted_row = sort_row(row)      # sorted row\n",
    "    \n",
    "    # find index of D0\n",
    "    index_D0 = sorted_row.ssd.index(day.isoformat())\n",
    "    indices_before_D0 = range(index_D0)\n",
    "    indices_after_D0 = range(index_D0+1, len(sorted_row.ssd))\n",
    "    \n",
    "    # there was activity prior to D0\n",
    "    if len(indices_before_D0) > 0:\n",
    "        try:\n",
    "            # get last entry from the row before D0\n",
    "            last_activity_date = sorted_row.ssd[indices_before_D0[-1]]\n",
    "            tsla = (day - datetime.strptime(last_activity_date, '%Y-%m-%d').date()).days\n",
    "        except:\n",
    "            # error in date format\n",
    "            tsla = -2\n",
    "    # maybe else check if num_ssd > 1 (active on same day), then tsla = 0?\n",
    "    \n",
    "    # there was activity after D0\n",
    "    if len(indices_after_D0) > 0:\n",
    "        try:\n",
    "            # get first entry from the row after D0\n",
    "            next_activity_date = sorted_row.ssd[indices_after_D0[0]]\n",
    "            tuna = (datetime.strptime(next_activity_date, '%Y-%m-%d').date() - day).days\n",
    "        except:\n",
    "            # error in date format\n",
    "            tuna = -2\n",
    "    # maybe else check if num_ssd > 1 (active on same day), then tuna = 0?\n",
    "\n",
    "    return (sorted_row.cid,              # cid\n",
    "            sorted_row.ssd,              # list of ssd (ascending)\n",
    "            sorted_row.num_ssd,          # list of num_ssd (based on ssd)\n",
    "            tsla,                        # time since last activity (days)\n",
    "            tuna,                        # time until next activity (days)\n",
    "           )\n",
    "\n",
    "def count_mapped_filter(table, f):\n",
    "    \"\"\"\n",
    "    This function applies a filter function to an rdd.\n",
    "    \n",
    "    @params:\n",
    "        - table [rdd] RDD to which we will apply the filter\n",
    "        - f [function] lambda function to apply the filter\n",
    "    \"\"\"\n",
    "    \n",
    "    return table.filter(f).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def write_col_json(fn, df_col, date_str, measure_str, cnt):\n",
    "    \"\"\"\n",
    "    This function writes the counter from a column from a dataframe to a json file.\n",
    "    \n",
    "    @params:\n",
    "        fn: [tring] file name of output json\n",
    "        df_col: [Series] column of the dataframe to create counter\n",
    "        date_str: [string] date to append to file name\n",
    "        measure_str: [string] name of measure (ex: \"tsla\", \"tuna\") to append to file name\n",
    "        cnt: [int] denominator for proportion calculation (ex: tsla_30_cnt, tuna_90_cnt)\n",
    "    \"\"\"\n",
    "    \n",
    "    cnter = dict(Counter(df_col))\n",
    "    inter = cnter.keys()\n",
    "    inter = [int(float(d)) for d in inter]\n",
    "    counts = cnter.values()\n",
    "\n",
    "    json_str = '[%s]'\n",
    "    row_str = '{\"days\":%s, \"count\":%s}'\n",
    "\n",
    "    all_rows = []\n",
    "    for i in range(len(inter)):\n",
    "        all_rows.append(row_str%(inter[i], counts[i]*100./cnt)) \n",
    "\n",
    "    all_rows_str = \",\".join(all_rows)\n",
    "\n",
    "    json_final = json_str%all_rows_str\n",
    "    \n",
    "    suffix = \"_\" + measure_str + \"-\" + date_str\n",
    "    file_name = fn + suffix + \".json\"\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        print \"{} exists, we will overwrite it.\".format(file_name)\n",
    "\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(json_final)\n",
    "\n",
    "def write_dict_json(fn, res_data, date_str):\n",
    "    \"\"\"\n",
    "    This function writes the content of a dictionary to a json file.\n",
    "    \n",
    "    @params:\n",
    "        fn: [string] file name of output json\n",
    "        res_data: [dict] dictionary object with summary data\n",
    "        date_str: [string] date to append to file name\n",
    "    \"\"\"\n",
    "    \n",
    "    suffix = \"-\" + date_str\n",
    "    file_name = fn + suffix + \".json\"\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        print \"{} exists, we will overwrite it.\".format(file_name)\n",
    "\n",
    "    # res_data is a JSON object.\n",
    "    json_entry = json.dumps(res_data)\n",
    "\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(\"[\" + json_entry.encode('utf8') + \"]\\n\")\n",
    "\n",
    "def make_dict_results(date_str, dau, tsla_neg_cnt, tuna_neg_cnt, tsla_tuna_neg_cnt, bad_dates_cnt,\n",
    "                      tsla_30_cnt, tuna_90_cnt):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary with a summary of the results to output a json file.\n",
    "    \n",
    "    @params:\n",
    "        keys to save as a dictionary. Self-explanatory\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"date\": date_str,\n",
    "        \"dau\": dau,\n",
    "        \"tsla_neg_prop\": tsla_neg_cnt*1./dau,\n",
    "        \"tuna_neg_prop\": tuna_neg_cnt*1./dau,\n",
    "        \"tsla_tuna_neg_prop\": tsla_tuna_neg_cnt*1./dau,\n",
    "        \"bad_dates_prop\": bad_dates_cnt*1./dau,\n",
    "        \"tsla_30_prop\": tsla_30_cnt*1./dau,\n",
    "        \"tuna_90_prop\": tuna_90_cnt*1./dau\n",
    "    }\n",
    "\n",
    "def file_exists(fn, date_str):\n",
    "    \"\"\"\n",
    "    This function determines whethera file already exists to prevent regenerating it.\n",
    "    \n",
    "    @params:\n",
    "        fn: [string] file name of output json\n",
    "        date_str: [string] date to append to file name\n",
    "    \"\"\"\n",
    "    \n",
    "    suffix = \"-\" + date_str\n",
    "    file_name = fn + suffix + \".json\"\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(start_backfill, end_backfill):\n",
    "    \"\"\"\n",
    "    This function runs all the program and is commented along the way.\n",
    "    It saves json files for each day of analysis within the specified range.\n",
    "    \n",
    "    @params:\n",
    "        start_backfill: [string] first date of analysis (YYYY-MM-DD)\n",
    "        end_backfill: [string] last date of analysis (YYYY-MM-DD)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check to see whether dates were provided\n",
    "    if (not start_backfill) or (not end_backfill):\n",
    "        print \"No dates provided\"\n",
    "        return\n",
    "    \n",
    "    # Import data from main summary\n",
    "    print \"***** READING DATA...\",\n",
    "    filteredPingsDF = read_main_summary()\n",
    "    filteredPingsDF_str = \"filteredPingsDF\"\n",
    "    sqlContext.registerDataFrameAsTable(filteredPingsDF, filteredPingsDF_str)\n",
    "    print \"Done!\"\n",
    "    \n",
    "    # Get dates for analysis\n",
    "    dates = [] # --list of all end_dates in period\n",
    "    print \"***** FINDING ALL days BETWEEN {start} AND {end}...\"\\\n",
    "           .format(start=start_backfill, end=end_backfill),\n",
    "\n",
    "    # Get all days between the two provided dates\n",
    "    delta = end_backfill - start_backfill\n",
    "    for i in range(delta.days + 1):\n",
    "        day = start_backfill + timedelta(days=i)\n",
    "        dates.append(day)\n",
    "    print \"{} DATES\".format(len(dates))\n",
    "    \n",
    "    # Iterate through all dates of analysis\n",
    "    for i, d in enumerate(dates):\n",
    "        print\n",
    "        print \"***** DATE {curr} of {tot}\".format(curr=i+1, tot=len(dates))\n",
    "        # Define and transform dates\n",
    "        D0 = d                                                  # date of of analysis (date)\n",
    "        D0_INT = date2int(D0)                                   # date of analysis (days since Jan 1st, 1970)\n",
    "\n",
    "        D0_T0 = D0 - timedelta(days=T0)                         # date beginning of latency period (date)\n",
    "        D0_T0_INT = D0_INT - T0                                 # date beginning of latency period (days since Jan 1st, 1970)\n",
    "\n",
    "        D0_T1 = D0 + timedelta(days=T1)                         # date beginning of come back period (date)\n",
    "        D0_T1_INT = D0_INT + T1                                 # date beginning of come back period (days since Jan 1st, 1970)\n",
    "        print \"***** WORKING ON DAY {}\".format(D0.isoformat())\n",
    "        \n",
    "        # Only do analysis if file does not currently exist\n",
    "        if not file_exists(\"fx_retusers\", D0.strftime(\"%Y%m%d\")):\n",
    "            # Aggregate by client and date\n",
    "            print \"\\t Aggregating data...\",\n",
    "            activeDF = get_active_on_date(filteredPingsDF_str, D0)\n",
    "            activeDF_str = \"activeDF\"\n",
    "            sqlContext.registerDataFrameAsTable(activeDF, activeDF_str)\n",
    "            print \"Done!\"\n",
    "\n",
    "            # Get DAU for D0\n",
    "            print \"\\t Getting DAU...\",\n",
    "            dau = activeDF.select(\"cid\").distinct().count()*100\n",
    "            print \"Done!\"\n",
    "\n",
    "            # Convert to longitudinal data set\n",
    "            print \"\\t Making longitudinal...\",\n",
    "            longitudinal = make_longitudinal(activeDF)\n",
    "            print \"Done!\"\n",
    "\n",
    "            # Apply mapping function to get tsla and tuna\n",
    "            print \"\\t Mapping data...\",\n",
    "            mapping_partial = partial(mapping, day = D0)\n",
    "            mapped = longitudinal.rdd.map(mapping_partial)\n",
    "            print \"Done!\"\n",
    "\n",
    "            # Get counts...\n",
    "            # ... for users not active before/after our interest, and potential badly formatted issues\n",
    "            print \"\\t Getting counts...\",\n",
    "            # Negative tsla <=> not active before D0\n",
    "            tsla_neg_cnt = count_mapped_filter(mapped, lambda row: row[3] < 0)*100\n",
    "            # Negative tuna <=> not active after D0\n",
    "            tuna_neg_cnt = count_mapped_filter(mapped, lambda row: row[4] < 0)*100\n",
    "            # Negative tsla and tuna <=> only active on D0\n",
    "            tsla_tuna_neg_cnt = count_mapped_filter(mapped, lambda row: (row[3] < 0 and row[4] < 0))*100\n",
    "            # One row has a badly formatted date at some point\n",
    "            # We can disregard it (or implement some better error checking at some point)\n",
    "            bad_dates_cnt = count_mapped_filter(mapped, lambda row: (row[3] == -2 or row[4] == -2))*100\n",
    "            print \"Done!\"\n",
    "\n",
    "            # ... for tsla (+ save counts for histogram)\n",
    "            print \"\\t Getting TSLA...\",\n",
    "            # tsla >= 30 <=> last time user was active was more than T0 (30) days before D0\n",
    "            tsla_30 = mapped.filter(lambda row: row[3] >= T0)\n",
    "            tsla_30_cnt = tsla_30.count()*100\n",
    "            tsla = tsla_30.map(lambda row: row[3]).collect()\n",
    "            write_col_json(\"fx_retusers\", tsla, D0.strftime(\"%Y%m%d\"), \"tsla\", tsla_30_cnt)\n",
    "            print \"Done!\"\n",
    "\n",
    "            # ... for tuna (+ save counts for histogram)\n",
    "            print \"\\t Getting TUNA...\",\n",
    "            # 0 < tuna <= 90 <=> next time user was active after D0\n",
    "            tuna_90 = mapped.filter(lambda row: (row[4] > 0 and row[4] <= T1))\n",
    "            tuna_90_cnt = tuna_90.count()*100\n",
    "            tuna = tuna_90.map(lambda row: row[4]).collect()\n",
    "            write_col_json(\"fx_retusers\", tuna, D0.strftime(\"%Y%m%d\"), \"tuna\", tuna_90_cnt)\n",
    "            print \"Done!\"\n",
    "\n",
    "            # Save results to json file\n",
    "            print \"\\t Saving json...\",\n",
    "            summary = make_dict_results(D0.isoformat(), dau, tsla_neg_cnt, tuna_neg_cnt, tsla_tuna_neg_cnt, bad_dates_cnt,\n",
    "                          tsla_30_cnt, tuna_90_cnt)\n",
    "            write_dict_json(\"fx_retusers\", summary, D0.strftime(\"%Y%m%d\"))\n",
    "            print \"Done!\"\n",
    "    \n",
    "            # Merge all json files to a single one\n",
    "            print \n",
    "            print \"Merging summary json files...\",\n",
    "            # merge summary JSON files into one\n",
    "            !jq -c -s \"[.[]|.[]]\" fx_retusers-*.json > \"fx_retusers.json\"\n",
    "            print \"DONE!\"\n",
    "        else:\n",
    "            print \"File exists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** READING DATA..."
     ]
    }
   ],
   "source": [
    "# Perform analysis from Sep 1st, 2016 to Dec 31st, 2016\n",
    "main(date(2016,9,1), date(2016,12,31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
