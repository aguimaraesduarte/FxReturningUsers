{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta, datetime\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "import os.path\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int2date(n):\n",
    "    \"\"\"\n",
    "    This function converts a number of days since Jan 1st 1970 <n> to a date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return date(1970,1,1)+timedelta(days=n)\n",
    "    except:\n",
    "        return date(1970,1,1)\n",
    "\n",
    "def date2int(d):\n",
    "    \"\"\"\n",
    "    This function converts a date <d> to number of days since Jan 1st 1970.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (d-date(1970,1,1)).days\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def str2date(s, f=\"%Y%m%d\"):\n",
    "    \"\"\"\n",
    "    This function converts a string <s> in the format <f> to a date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(s, f).date()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "TODAY = date.today()                                    # today (date)\n",
    "TODAY_INT = date2int(TODAY)                             # today (days since Jan 1st, 1970)\n",
    "D0 = date(2016, 12, 12)                                 # start date of week of analysis (date)\n",
    "D0_INT = date2int(D0)                                   # start date of week of analysis (days since Jan 1st, 1970)\n",
    "PCD_CUTOUT_START_DATE = date(2010, 1, 1)                # profiles created before this date are removed (date)\n",
    "PCD_CUTOUT_START_INT = date2int(PCD_CUTOUT_START_DATE)  # profiles created before this date are removed (days since Jan 1st, 1970)\n",
    "T0 = 180                                                # time to consider stop using Fx\n",
    "D0_T0 = D0 - timedelta(days=T0)                         # date beginning of stoppage period (date)\n",
    "D0_T0_INT = D0_INT - T0                                 # date beginning of stoppage period (days since Jan 1st, 1970)\n",
    "D6 = D0 + timedelta(days=6)                             # date end of week of analysis (date)\n",
    "D6_INT = D0_INT + 6                                     # date end of week of analysis (days since Jan 1st, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_main_summary():\n",
    "    \"\"\"\n",
    "    This function imports the main_summary dataset from S3, selects the variables of interest,\n",
    "    applies several filters, and returns the filtered dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # connect to the main summary dataset\n",
    "    allPingsDF = sqlContext.read.load(\"s3://telemetry-parquet/main_summary/v3\", \"parquet\", mergeSchema=True)\n",
    "    \n",
    "    # perform variable selection with column renaming\n",
    "    allPingsDFSelect = allPingsDF.select(\n",
    "               allPingsDF.client_id.alias(\"cid\"),\n",
    "               allPingsDF.sample_id.alias(\"sid\"),\n",
    "               allPingsDF.normalized_channel.alias(\"channel\"),\n",
    "               allPingsDF.subsession_start_date.alias(\"ssd\"),\n",
    "               allPingsDF.app_name.alias(\"appname\"),\n",
    "               allPingsDF.subsession_length.alias(\"ssl\"),\n",
    "               allPingsDF.profile_creation_date.alias(\"pcd\"),\n",
    "               allPingsDF.app_version.alias(\"av\"))\n",
    "\n",
    "    # filter, replace missing values with zeroes, and cache dataframe\n",
    "    # - 1% sample (sample_id is 42)\n",
    "    # - channel is release\n",
    "    # - application is Firefox\n",
    "    # - subsession length is positive and less than 24h\n",
    "    # - profile creation date is after Jan 1st, 2010 and before TODAY, either before D0-T0 or after D0 (during week of analysis)\n",
    "    # Dataframe is `cache`d to memory for performance improvements\n",
    "    filteredPingsDF = allPingsDFSelect.filter(allPingsDFSelect.sid == \"42\")\\\n",
    "                                      .filter(allPingsDFSelect.channel == \"release\")\\\n",
    "                                      .filter(allPingsDFSelect.appname == \"Firefox\")\\\n",
    "                                      .filter(allPingsDFSelect.ssl >= 0)\\\n",
    "                                      .filter(allPingsDFSelect.ssl <= 86400)\\\n",
    "                                      .filter(allPingsDFSelect.pcd >= PCD_CUTOUT_START_INT)\\\n",
    "                                      .filter(allPingsDFSelect.pcd <= TODAY_INT)\\\n",
    "                                      .filter(~allPingsDFSelect.pcd.isin(range(D0_T0_INT, D0_INT)))\\\n",
    "                                      .select([\"cid\",\"ssd\",\"pcd\",\"ssl\",\"av\"])\\\n",
    "                                      .cache()\n",
    "                                        \n",
    "    # return filtered dataset\n",
    "    return filteredPingsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_longitudinal(df):\n",
    "    \"\"\"\n",
    "    This function creates and returns a longitudinal dataframe from a dataframe.\n",
    "    Each Row from this dataframe contains the sequential information (lists):\n",
    "        - subsession_length (ssl),\n",
    "        - subsession_start_date (ssd),\n",
    "        - profile_creation_date (pcd),\n",
    "        - app_version (av)\n",
    "    for each cid.\n",
    "    \n",
    "    @params:\n",
    "        df: [dataframe] dataframe returned by read_main_summary()\n",
    "    \"\"\"\n",
    "    \n",
    "    longitudinal = df.groupBy(\"cid\")\\\n",
    "                     .agg({\"ssl\": \"collect_list\",\n",
    "                           \"ssd\": \"collect_list\",\n",
    "                           \"av\": \"collect_list\",\n",
    "                           \"pcd\": \"first\"})\\\n",
    "                     .withColumnRenamed(\"first(pcd)\",\"pcd\")\\\n",
    "                     .withColumnRenamed(\"collect_list(ssl)\",\"ssl\")\\\n",
    "                     .withColumnRenamed(\"collect_list(ssd)\",\"ssd\")\\\n",
    "                     .withColumnRenamed(\"collect_list(av)\",\"av\")\n",
    "    \n",
    "    return longitudinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapping(row):\n",
    "    \"\"\"\n",
    "    Applied to an RDD, this mapping function returns a tuple of seven elements for each Row of the dataframe:\n",
    "        - cohort: [string] One of {'no: active s','no: inactive b','no: inactive w',\n",
    "                                   'no: pcd in s','yes: active b, inactive s, active w','yes: pcd in w'}\n",
    "        - cid: [string] Client_id\n",
    "        - pcd: [int] Profile creation date (in days since Jan 1st, 1970)\n",
    "        - ssd: [list] List of all subsession start dates (ascending)\n",
    "        - ssl: [list] List of all subsession lengths (ordered with repect to ssd) \n",
    "        - pcd_date: [date] Profile creation date (as datetime.date)\n",
    "        - num_ssd_in_w: [int] Number of subsessions that started in the week of interest\n",
    "        - tot_ssl_in_w: [int] Sum of subsession lengths within the week of interest\n",
    "        - num_ssd_in_b: [int] Number of subsessions before D0-T0\n",
    "        - tot_ssl_in_b: [int] Sum of subsession lengths before D0-T0\n",
    "    \n",
    "    @params:\n",
    "        row: [Row] a row from a longitudinal RDD that includes:\n",
    "            - cid: client_id\n",
    "            - ssl: subsession_length\n",
    "            - ssd: subsession_start_date\n",
    "            - pcd: profile_creation_date\n",
    "            - av: app_version\n",
    "    \n",
    "    @logic:\n",
    "        - sort the row based on ssd (ascending). Keep only entries up to D6 (end of week of analysis)\n",
    "        - if pcd within the week: update cohort (\"yes\")\n",
    "        - else\n",
    "            - if user was\n",
    "                - not active during the week of analysis,\n",
    "                - OR active during [D0-T0, D0],\n",
    "                - OR not active before D0-T0\n",
    "                - THEN update cohort (\"no\")\n",
    "            - else: update cohort (\"yes\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def sort_row(row):\n",
    "        # sort ssd and ssl by descending ssd\n",
    "        zipped = sorted(zip(row.ssd, row.ssl, row.av), reverse=False)\n",
    "        ssd, ssl, av = zip(*zipped)\n",
    "        \n",
    "        # keep only subsessions that started before the end of the week of interest\n",
    "        ssd = [s[:10] for s in ssd if s[:10] <= D6.strftime(\"%Y-%m-%d\")]\n",
    "        \n",
    "        # trim ssl, av (since it was sorted in ascending fashion, trim the end)\n",
    "        ssl = ssl[:len(ssd)]\n",
    "        av = av[:len(ssd)]\n",
    "        \n",
    "        # indices within week of analysis\n",
    "        indices_in_w = [i for i in range(len(row.ssd)) if row.ssd[i][:10]<=D6.strftime(\"%Y-%m-%d\") and row.ssd[i][:10]>=D0.strftime(\"%Y-%m-%d\")]\n",
    "        # indices before D0-T0\n",
    "        indices_in_b = [i for i in range(len(row.ssd)) if row.ssd[i][:10]<=D0_T0.strftime(\"%Y-%m-%d\")]\n",
    "        \n",
    "        # ssd, ssl, av for each period\n",
    "        ssd_in_w = [row.ssd[i][:10] for i in indices_in_w]\n",
    "        ssl_in_w = [row.ssl[i] for i in indices_in_w]\n",
    "        av_in_w = [row.av[i] for i in indices_in_w]\n",
    "        ssd_in_b = [row.ssd[i][:10] for i in indices_in_b]\n",
    "        ssl_in_b = [row.ssl[i] for i in indices_in_b]\n",
    "        av_in_b = [row.av[i] for i in indices_in_b]\n",
    "        \n",
    "        # return reformatted Row with additional info\n",
    "        return Row(cid=row.cid,\n",
    "                   pcd=row.pcd,\n",
    "                   ssd=list(ssd),\n",
    "                   ssl=list(ssl),\n",
    "                   av=list(av),\n",
    "                   av_in_w=av_in_w,\n",
    "                   av_in_b=av_in_b,\n",
    "                   pcd_date=int2date(row.pcd),\n",
    "                   ssd_in_w=ssd_in_w,\n",
    "                   ssd_in_b=ssd_in_b,\n",
    "                   num_ssd_in_w=len(ssd_in_w),\n",
    "                   tot_ssl_in_w=sum(ssl_in_w),\n",
    "                   num_ssd_in_b=len(ssd_in_b),\n",
    "                   tot_ssl_in_b=sum(ssl_in_b))\n",
    "    \n",
    "    cohort = \"\"\n",
    "    last_version_in_b = \"NA\"\n",
    "    first_version_in_w = \"NA\"\n",
    "    days_in_s = 0\n",
    "    \n",
    "    s_row = sort_row(row)\n",
    "    \n",
    "    if s_row.pcd >= D0_INT and s_row.pcd <= D6_INT:\n",
    "        cohort = \"yes: pcd in w\"\n",
    "        #first_version_in_w = s_row.av_in_w[0]\n",
    "    elif s_row.pcd <= D0_T0_INT:\n",
    "        week = [(D0 + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((D6-D0).days + 1)]\n",
    "        stagnant = [(D0_T0 + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(1, (D0-D0_T0).days)]\n",
    "        beginning = [(s_row.pcd_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((D0_T0-s_row.pcd_date).days +1)]\n",
    "        set_ssd = set(s_row.ssd)\n",
    "        if len(set(week) & set_ssd) == 0: # not active during week\n",
    "            cohort = \"no: inactive w\"\n",
    "        elif len(set(stagnant) & set_ssd) > 0: # active during stagnant\n",
    "            cohort = \"no: active s\"\n",
    "        elif len(set(beginning) & set_ssd) == 0: # not active during beginning\n",
    "            cohort = \"no: inactive b\"\n",
    "        else:\n",
    "            cohort = \"yes: active b, inactive s, active w\"\n",
    "            last_version_in_b = s_row.av_in_b[-1]\n",
    "            first_version_in_w = s_row.av_in_w[0]\n",
    "            last_day_in_b = s_row.ssd_in_b[-1]\n",
    "            first_day_in_w = s_row.ssd_in_w[0]\n",
    "            days_in_s = (datetime.strptime(first_day_in_w, \"%Y-%m-%d\") - datetime.strptime(last_day_in_b, \"%Y-%m-%d\")).days\n",
    "    else:\n",
    "        cohort = \"no: pcd in s\"\n",
    "    \n",
    "    return (cohort,                 # cohort\n",
    "            last_version_in_b,      # last Fx version in beginning\n",
    "            first_version_in_w,     # first Fx version in week of interest\n",
    "            s_row.cid,              # cid\n",
    "            s_row.pcd,              # pcd (days since Jan 1st, 1970)\n",
    "            s_row.ssd,              # list of ssd (ascending)\n",
    "            s_row.ssl,              # list of ssl (based on ssd)\n",
    "            s_row.av,               # list of av (based on ssd)\n",
    "            s_row.pcd_date,         # pcd (date)\n",
    "            s_row.num_ssd_in_w,     # number of days in week of analysis\n",
    "            s_row.tot_ssl_in_w,     # total ssl in week of analysis\n",
    "            s_row.num_ssd_in_b,     # number of days before D0-T0\n",
    "            s_row.tot_ssl_in_b,     # total ssl before D0-T0\n",
    "            days_in_s)              # days spent without use of Fx\n",
    "\n",
    "def get_positives(mapped_rdd):\n",
    "    return mapped_rdd.filter(lambda row: row[0].startswith(\"yes\"))\n",
    "\n",
    "def get_cohort(positives_rdd, condition=\"\"):\n",
    "    return positives_rdd.filter(lambda row: row[0] == condition)\n",
    "\n",
    "def toDF(positives_rdd, cols = [\"cohort\",\"last_version_in_b\",\"first_version_in_w\",\"cid\",\"pcd\",\n",
    "                                \"ssd\",\"ssl\",\"av\",\"pcd_date\",\"num_ssd_in_w\",\"tot_ssl_in_w\",\n",
    "                                \"num_ssd_in_b\",\"tot_ssl_in_b\",\"days_in_s\"]):\n",
    "    return positives_rdd.toDF(cols)\n",
    "\n",
    "def upgraded_not_upgraded(df):\n",
    "    return (df[df.last_version_in_b != df.first_version_in_w], df[df.last_version_in_b == df.first_version_in_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_dict_json(fn, res_data, start_date_str, end_date_str):\n",
    "    \"\"\"\n",
    "    This function writes the content of a dictionary to a json file.\n",
    "    \n",
    "    @params:\n",
    "        fn: [string] file name of output json\n",
    "        res_data: [dict] dictionary object with summary data\n",
    "        start_date_str: [string] start date to append to file name\n",
    "        end_date_str: [string] end date to append to file names\n",
    "    \"\"\"\n",
    "    \n",
    "    suffix = \"-\" + start_date_str + \"-\" + end_date_str\n",
    "    file_name = fn + suffix + \".json\"\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        print \"{} exists, we will overwrite it.\".format(file_name)\n",
    "\n",
    "    # res_data is a JSON object.\n",
    "    json_entry = json.dumps(res_data)\n",
    "\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(\"[\" + json_entry.encode('utf8') + \"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_alg(start_backfill=None, end_backfill=None):\n",
    "    \"\"\"\n",
    "    This function ties everything together.\n",
    "    If no date parameters are specified, the analysis is done for the week ending today-14 days.\n",
    "    If both date parameters are specified (as \"%Y-%m-%d\" strings), the program will find all\n",
    "        Sundays in-between the two dates and run the algorithm for all weeks ending on those days.\n",
    "        This is used for backfilling data only!\n",
    "    \n",
    "    @params:\n",
    "        start_backfill: [string \"%Y-%m-%d\"] first end_date for backfilling\n",
    "        end_backfill: [string \"%Y-%m-%d\"] last end_date for backfilling\n",
    "        ex: start_backfill=\"2016-10-01\" and end_backfill=\"2017-01-01\" will produce data for **all** Sundays\n",
    "            between October 1st 2016 and January 1st 2017\n",
    "    \"\"\"\n",
    "    \n",
    "    # read and clean data; save as SQL table\n",
    "    print \"***** READING DATA...\",\n",
    "    filteredPingsDF = read_main_summary()\n",
    "    print \"DONE!\"\n",
    "\n",
    "    # get date ranges\n",
    "    dates = [] # --list of all end_dates in period\n",
    "    if start_backfill and end_backfill:\n",
    "        print \"***** BACKFILL DATES PROVIDED\"\n",
    "        print \"***** FINDING ALL Sundays BETWEEN {start} AND {end}...\"\\\n",
    "               .format(start=start_backfill, end=end_backfill),\n",
    "        # get all Sundays between the two provided dates\n",
    "        d1 = datetime.strptime(start_backfill, \"%Y-%m-%d\").date()\n",
    "        d2 = datetime.strptime(end_backfill, \"%Y-%m-%d\").date()\n",
    "\n",
    "        delta = d2 - d1\n",
    "        for i in range(delta.days + 1):\n",
    "            day = d2 - timedelta(days=i)\n",
    "            if day.weekday() in [6]:\n",
    "                end_date = day\n",
    "                start_date = day - timedelta(days=6)\n",
    "                dates.append( (start_date, end_date) )\n",
    "        print \"{} DATES\".format(len(dates))\n",
    "    else:\n",
    "        # only use week ending today - 14 days\n",
    "        end_date = date.today() - timedelta(days=14)\n",
    "        start_date = end_date - timedelta(days=6)\n",
    "        dates.append( (start_date, end_date) )\n",
    "\n",
    "    # loop through all dates\n",
    "    for i, d in enumerate(dates):\n",
    "        print\n",
    "        print \"***** DATE {curr} of {tot}\".format(curr=i+1, tot=len(dates))\n",
    "        start_date = d[0]\n",
    "        end_date = d[1]\n",
    "        start_date_str = start_date.strftime(\"%Y%m%d\")\n",
    "        end_date_str = end_date.strftime(\"%Y%m%d\")\n",
    "        print \"***** Week of interest: {start} :: {end}\".format(start=start_date, end=end_date)\n",
    "\n",
    "        # transform into longitudinal format\n",
    "        longitudinal = make_longitudinal(filteredPingsDF)\n",
    "\n",
    "        # apply mapping function\n",
    "        mapped = longitudinal.rdd.map(mapping)\n",
    "\n",
    "        # get counts of cohorts\n",
    "        cohorts = mapped.countByKey()\n",
    "        print \"\\tNumber of profiles that created their profile in the week of analysis:\"\n",
    "        print \"\\t\\t{:,}\".format(cohorts[\"yes: pcd in w\"]*100)\n",
    "        print \"\\tNumber of profiles that were active before {}, inactive until {},\\n\\tand active in the week of analysis:\"\\\n",
    "              .format(D0_T0.isoformat(), D0.isoformat())\n",
    "        print \"\\t\\t{:,}\".format(cohorts[\"yes: active b, inactive s, active w\"]*100)\n",
    "        \n",
    "        # get positives, convert to DF\n",
    "        print \"\\tConverting to DF...\",\n",
    "        positives_rdd = get_positives(mapped)\n",
    "        cohort1_rdd = get_cohort(positives_rdd, \"yes: active b, inactive s, active w\")\n",
    "        cohort2_rdd = get_cohort(positives_rdd, \"yes: pcd in w\")\n",
    "        print \"Done!\",\n",
    "        positives_df = toDF(positives_rdd)\n",
    "        cohort1_df = toDF(cohort1_rdd)\n",
    "        cohort2_df = toDF(cohort2_rdd)\n",
    "        print \"Done!\",\n",
    "        upgraded, not_upgraded = upgraded_not_upgraded(cohort1_df)\n",
    "        print \"Done!\"\n",
    "        \n",
    "        # write to dict\n",
    "        write_dict_json(\"fx_newusers\", cohorts, start_date_str, end_date_str)\n",
    "        print \"DONE!\"\n",
    "\n",
    "    print \n",
    "    print \"**** MERGING SUMMARY JSON FILES...\",\n",
    "    # merge summary JSON files into one\n",
    "    !jq -c -s \"[.[]|.[]]\" fx_newusers-*.json > \"fx_newusers.json\"\n",
    "    print \"DONE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** READING DATA... DONE!\n",
      "\n",
      "***** DATE 1 of 1\n",
      "***** Week of interest: 2017-02-07 :: 2017-02-13\n",
      "\tNumber of profiles that created their profile in the week of analysis:\n",
      "\t\t9,186,100\n",
      "\tNumber of profiles that were active before 2016-06-15, inactive until 2016-12-12,\n",
      "\tand active in the week of analysis:\n",
      "\t\t652,100\n",
      "\tConverting to DF... Done! Done! Done!\n",
      "DataFrame[cohort: string, last_version_in_b: string, first_version_in_w: string, cid: string, pcd: bigint, ssd: array<string>, ssl: array<bigint>, av: array<string>, pcd_date: date, num_ssd_in_w: bigint, tot_ssl_in_w: bigint, num_ssd_in_b: bigint, tot_ssl_in_b: bigint, days_in_s: bigint]\n",
      "DataFrame[cohort: string, last_version_in_b: string, first_version_in_w: string, cid: string, pcd: bigint, ssd: array<string>, ssl: array<bigint>, av: array<string>, pcd_date: date, num_ssd_in_w: bigint, tot_ssl_in_w: bigint, num_ssd_in_b: bigint, tot_ssl_in_b: bigint, days_in_s: bigint]\n",
      "DONE!\n",
      "\n",
      "**** MERGING SUMMARY JSON FILES... DONE!\n"
     ]
    }
   ],
   "source": [
    "main_alg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** READING DATA... DONE!\n",
      "***** BACKFILL DATES PROVIDED\n",
      "***** FINDING ALL Sundays BETWEEN 2016-09-01 AND 2016-10-01... 4 DATES\n",
      "\n",
      "***** DATE 1 of 4\n",
      "***** Week of interest: 2016-09-19 :: 2016-09-25\n"
     ]
    }
   ],
   "source": [
    "#main_alg(\"2016-09-01\", \"2016-10-01\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
